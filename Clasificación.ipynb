{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clasificación de series de tiempo\n",
    "\n",
    "- Elegir **una de las siguientes opciones**:\n",
    "  - Transformar el problema de regresión abordado previamente en un problema de clasificación (por ejemplo, clasificar tendencias como \"sube\", \"baja\" o \"estable\").\n",
    "  - Seleccionar **una nueva base de datos** específicamente orientada a clasificación de series de tiempo.\n",
    "\n",
    "- Implementar las siguientes estructuras de modelos que permitan resolver el problema de clasificación:\n",
    "  - MLP para clasificación\n",
    "  - CNN para clasificación\n",
    "  - LSTM para clasificación\n",
    "  - CNN-LSTM para clasificación\n",
    "  - Algoritmos clásicos de Machine Learning (SVM, Random Forest, etc.)\n",
    "\n",
    "https://iris.who.int/bitstream/handle/10665/345329/9789240034228-eng.pdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Conv1D, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import optuna\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from mlflow_runs import MLflowCallback\n",
    "mlflow_callback = MLflowCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-03-10 18:00:00</th>\n",
       "      <td>2.6</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>48.9</td>\n",
       "      <td>0.7578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-10 19:00:00</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>955.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>47.7</td>\n",
       "      <td>0.7255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-10 20:00:00</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.7502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-10 21:00:00</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>948.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.7867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-10 22:00:00</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>836.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>0.7888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-04 10:00:00</th>\n",
       "      <td>3.1</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>1729.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-04 11:00:00</th>\n",
       "      <td>2.4</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.7119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-04 12:00:00</th>\n",
       "      <td>2.4</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.6406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-04 13:00:00</th>\n",
       "      <td>2.1</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>961.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-04 14:00:00</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.5028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9357 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  \\\n",
       "Datetime                                                            \n",
       "2004-03-10 18:00:00     2.6       1360.0      11.9         1046.0   \n",
       "2004-03-10 19:00:00     2.0       1292.0       9.4          955.0   \n",
       "2004-03-10 20:00:00     2.2       1402.0       9.0          939.0   \n",
       "2004-03-10 21:00:00     2.2       1376.0       9.2          948.0   \n",
       "2004-03-10 22:00:00     1.6       1272.0       6.5          836.0   \n",
       "...                     ...          ...       ...            ...   \n",
       "2005-04-04 10:00:00     3.1       1314.0      13.5         1101.0   \n",
       "2005-04-04 11:00:00     2.4       1163.0      11.4         1027.0   \n",
       "2005-04-04 12:00:00     2.4       1142.0      12.4         1063.0   \n",
       "2005-04-04 13:00:00     2.1       1003.0       9.5          961.0   \n",
       "2005-04-04 14:00:00     2.2       1071.0      11.9         1047.0   \n",
       "\n",
       "                     PT08.S3(NOx)  PT08.S4(NO2)  PT08.S5(O3)     T    RH  \\\n",
       "Datetime                                                                   \n",
       "2004-03-10 18:00:00        1056.0        1692.0       1268.0  13.6  48.9   \n",
       "2004-03-10 19:00:00        1174.0        1559.0        972.0  13.3  47.7   \n",
       "2004-03-10 20:00:00        1140.0        1555.0       1074.0  11.9  54.0   \n",
       "2004-03-10 21:00:00        1092.0        1584.0       1203.0  11.0  60.0   \n",
       "2004-03-10 22:00:00        1205.0        1490.0       1110.0  11.2  59.6   \n",
       "...                           ...           ...          ...   ...   ...   \n",
       "2005-04-04 10:00:00         539.0        1374.0       1729.0  21.9  29.3   \n",
       "2005-04-04 11:00:00         604.0        1264.0       1269.0  24.3  23.7   \n",
       "2005-04-04 12:00:00         603.0        1241.0       1092.0  26.9  18.3   \n",
       "2005-04-04 13:00:00         702.0        1041.0        770.0  28.3  13.5   \n",
       "2005-04-04 14:00:00         654.0        1129.0        816.0  28.5  13.1   \n",
       "\n",
       "                         AH  \n",
       "Datetime                     \n",
       "2004-03-10 18:00:00  0.7578  \n",
       "2004-03-10 19:00:00  0.7255  \n",
       "2004-03-10 20:00:00  0.7502  \n",
       "2004-03-10 21:00:00  0.7867  \n",
       "2004-03-10 22:00:00  0.7888  \n",
       "...                     ...  \n",
       "2005-04-04 10:00:00  0.7568  \n",
       "2005-04-04 11:00:00  0.7119  \n",
       "2005-04-04 12:00:00  0.6406  \n",
       "2005-04-04 13:00:00  0.5139  \n",
       "2005-04-04 14:00:00  0.5028  \n",
       "\n",
       "[9357 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/air_quality_clean.csv\", parse_dates=['Datetime'])\n",
    "df.set_index('Datetime', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vamos a hacer lo de bajo, medio y alto nivel de monóxido de cárbono en el aire. Para esto investigaramos cuales suelen ser los niveles en el aire para poderlos clasificar. \n",
    "Según la OMS (Organización Mundial de la Salud) y otras agencias como la EPA (USA):\n",
    "\n",
    "| Nivel de CO  | Rango aproximado (mg/m³) |\n",
    "| ------------ | ------------------------ |\n",
    "| **Bajo**     | 0 – 4.4                  |\n",
    "| **Moderado** | 4.5 – 9.0 >              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_binario(valores):\n",
    "    clases = []\n",
    "    for val in valores:\n",
    "        if val <= 4.4:\n",
    "            clases.append(0)  # Normal\n",
    "        else:\n",
    "            clases.append(1)  # No normal (Moderado o Alto)\n",
    "    return np.array(clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 es baja\n",
    "# 1 es no normal, ya sea moderado o alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = clasificar_binario(df[\"CO(GT)\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Clase  Cantidad   Etiqueta\n",
      "0      0      8772     Normal\n",
      "1      1       585  No normal\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_bin, return_counts=True)\n",
    "\n",
    "conteo_df = pd.DataFrame({'Clase': unique, 'Cantidad': counts})\n",
    "conteo_df['Etiqueta'] = conteo_df['Clase'].map({0: 'Normal', 1: 'No normal'})\n",
    "\n",
    "print(conteo_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habiamos intententado que las clases fueran, bajo, moderado y alto. Pero como de alto solo habia 12 valores, lo integramos a una que se llamara \"no Normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = clasificar_binario(df[\"CO(GT)\"].values)\n",
    "X = df.drop(columns=[\"CO(GT)\"]).values # todas menos la variable a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ventanas de tiempo aplanadas \n",
    "def crear_ventanas_bin(X, y, window_size):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window_size, len(y)):\n",
    "        Xs.append(X[i-window_size:i].flatten())\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "window_size = 12\n",
    "X_seq, y_seq = crear_ventanas_bin(X_scaled, y_bin, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.2050 - val_accuracy: 0.8636 - val_loss: 0.3496\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - accuracy: 0.9560 - loss: 0.1170 - val_accuracy: 0.8817 - val_loss: 0.3135\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.9589 - loss: 0.1071 - val_accuracy: 0.8850 - val_loss: 0.3098\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - accuracy: 0.9580 - loss: 0.1083 - val_accuracy: 0.8951 - val_loss: 0.3262\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.9513 - loss: 0.1156 - val_accuracy: 0.9098 - val_loss: 0.3068\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - accuracy: 0.9583 - loss: 0.1079 - val_accuracy: 0.9064 - val_loss: 0.3025\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 0.9593 - loss: 0.1026 - val_accuracy: 0.8964 - val_loss: 0.3148\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.9631 - loss: 0.0918 - val_accuracy: 0.8957 - val_loss: 0.3035\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - accuracy: 0.9568 - loss: 0.0999 - val_accuracy: 0.8890 - val_loss: 0.3151\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - accuracy: 0.9536 - loss: 0.1018 - val_accuracy: 0.8910 - val_loss: 0.3232\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.9608 - loss: 0.1045 - val_accuracy: 0.8770 - val_loss: 0.3783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17cc57650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo MLP \n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # salida binaria\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# EarlyStopping \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenamiento\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.2,\n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          callbacks=[early_stop, mlflow_callback],\n",
    "          verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluación\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación binaria:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.97      0.97      0.97      1756\n",
      "   No normal       0.57      0.59      0.58       113\n",
      "\n",
      "    accuracy                           0.95      1869\n",
      "   macro avg       0.77      0.78      0.78      1869\n",
      "weighted avg       0.95      0.95      0.95      1869\n",
      "\n",
      "Exactitud total: 0.948\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación binaria:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo detecta muy bien los casos normales, con precisión y recall cercanos al 98%.\n",
    "* En los casos \"No normales\" (niveles elevados de CO), el desempeño baja:\n",
    "\n",
    "Precisión de 60% → cuando predice “no normal”, acierta el 60% de las veces.\n",
    "\n",
    "Recall de 54% → detecta poco más de la mitad de los casos reales de “no normal”.\n",
    "\n",
    "Esto es normal ya que las clases estaban desbalanceadas.\n",
    "\n",
    "El modelo es muy confiable para identificar niveles normales de CO(GT), pero tiene dificultades para detectar eventos poco frecuentes de contaminación elevada. Aun así, puede servir como un buen sistema de alerta preliminar, especialmente si se complementa con técnicas de balanceo de clases o ajuste de pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ventanas multivariadas para CNN \n",
    "def crear_ventanas_cnn(X, y, window_size):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window_size, len(y)):\n",
    "        Xs.append(X[i-window_size:i, :])  \n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "window_size = 24\n",
    "X_seq, y_seq = crear_ventanas_cnn(X_scaled, y_bin, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7466, 24, 9)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9240 - loss: 0.2461 - val_accuracy: 0.9347 - val_loss: 0.1724\n",
      "Epoch 2/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.9431 - loss: 0.1576 - val_accuracy: 0.9405 - val_loss: 0.1677\n",
      "Epoch 3/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - accuracy: 0.9412 - loss: 0.1469 - val_accuracy: 0.9459 - val_loss: 0.1499\n",
      "Epoch 4/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9500 - loss: 0.1285 - val_accuracy: 0.9486 - val_loss: 0.1407\n",
      "Epoch 5/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - accuracy: 0.9515 - loss: 0.1278 - val_accuracy: 0.9475 - val_loss: 0.1355\n",
      "Epoch 6/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1266 - val_accuracy: 0.9470 - val_loss: 0.1354\n",
      "Epoch 7/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.9492 - loss: 0.1266 - val_accuracy: 0.9513 - val_loss: 0.1305\n",
      "Epoch 8/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - accuracy: 0.9571 - loss: 0.1051 - val_accuracy: 0.9523 - val_loss: 0.1272\n",
      "Epoch 9/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9515 - loss: 0.1167 - val_accuracy: 0.9523 - val_loss: 0.1242\n",
      "Epoch 10/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - accuracy: 0.9553 - loss: 0.1138 - val_accuracy: 0.9534 - val_loss: 0.1220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17e45b890>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
    "\n",
    "# 1. Crear ventanas\n",
    "window_size = 24\n",
    "X_seq, y_seq = crear_ventanas_cnn(X_scaled, y_bin, window_size)\n",
    "\n",
    "# 2. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Verificar shapes\n",
    "print(\"X_train:\", X_train.shape)  # (n_train, 24, n_features)\n",
    "\n",
    "# 4. Definir modelo\n",
    "timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(timesteps, n_features)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Entrenar\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluar \n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la probabilidad es mayor a 0.5, se predice la clase 1 (No normal).\n",
    "\n",
    "Si es menor o igual a 0.5, se predice la clase 0 (Normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación CNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.97      0.98      0.98      1742\n",
      "   No normal       0.70      0.53      0.60       125\n",
      "\n",
      "    accuracy                           0.95      1867\n",
      "   macro avg       0.83      0.76      0.79      1867\n",
      "weighted avg       0.95      0.95      0.95      1867\n",
      "\n",
      "Exactitud total: 0.953\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación CNN:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos modelos clasifican muy bien la clase “Normal”, como es de esperarse por el desbalance de las clases.\n",
    "\n",
    "El CNN tiene mayor precisión en detectar \"No normal\", pero mucho menor recall (predice pocos positivos, pero con menos error).\n",
    "\n",
    "El MLP tiene mejor balance entre precisión y recall, por lo tanto mejor F1-score para la clase minoritaria.\n",
    "\n",
    "La exactitud total es prácticamente la misma en ambos casos, pero no refleja el verdadero rendimiento sobre la clase difícil (\"No normal\").\n",
    "\n",
    "Si nos importara  no pasar por alto ningún caso de monóxido elevado, el MLP es mejor (más recall).\n",
    "\n",
    "Si preferimos menos falsos positivos (es decir, cuando diga “No normal” esté bien seguro), el CNN puede servir mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ventanas multivariadas para LSTM \n",
    "def crear_ventanas_lstm(X, y, window_size):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window_size, len(y)):\n",
    "        Xs.append(X[i-window_size:i, :])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "window_size = 24\n",
    "X_seq, y_seq = crear_ventanas_lstm(X_scaled, y_bin, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Modelo LSTM \n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # sigmoide por ser binaria\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8975 - loss: 0.3213 - val_accuracy: 0.9391 - val_loss: 0.2074\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9403 - loss: 0.1993 - val_accuracy: 0.9398 - val_loss: 0.1725\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9380 - loss: 0.1782 - val_accuracy: 0.9444 - val_loss: 0.1492\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9441 - loss: 0.1562 - val_accuracy: 0.9404 - val_loss: 0.1394\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9507 - loss: 0.1354 - val_accuracy: 0.9444 - val_loss: 0.1305\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9488 - loss: 0.1416 - val_accuracy: 0.9444 - val_loss: 0.1273\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9517 - loss: 0.1387 - val_accuracy: 0.9431 - val_loss: 0.1320\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9523 - loss: 0.1327 - val_accuracy: 0.9438 - val_loss: 0.1239\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9488 - loss: 0.1305 - val_accuracy: 0.9498 - val_loss: 0.1235\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9510 - loss: 0.1258 - val_accuracy: 0.9378 - val_loss: 0.1314\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9517 - loss: 0.1284 - val_accuracy: 0.9444 - val_loss: 0.1221\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9459 - loss: 0.1379 - val_accuracy: 0.9458 - val_loss: 0.1215\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9554 - loss: 0.1199 - val_accuracy: 0.9471 - val_loss: 0.1237\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9540 - loss: 0.1228 - val_accuracy: 0.9458 - val_loss: 0.1241\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9525 - loss: 0.1304 - val_accuracy: 0.9525 - val_loss: 0.1224\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1330 - val_accuracy: 0.9465 - val_loss: 0.1254\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9515 - loss: 0.1318 - val_accuracy: 0.9444 - val_loss: 0.1282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x309391e80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EarlyStopping \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenamiento\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, mlflow_callback],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluar\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      0.99      0.97      1742\n",
      "   No normal       0.81      0.34      0.48       125\n",
      "\n",
      "    accuracy                           0.95      1867\n",
      "   macro avg       0.88      0.67      0.73      1867\n",
      "weighted avg       0.95      0.95      0.94      1867\n",
      "\n",
      "Exactitud total: 0.951\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación LSTM:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo LSTM clasifica muy bien la clase \"Normal\", como todos los anteriores.\n",
    "\n",
    "En la clase \"No normal\", tiene un buen equilibrio entre precisión (62%) y recall (50%), lo que resulta en un F1-score razonable (0.55).\n",
    "\n",
    "Mejora el recall respecto al CNN (que fue solo del 27%) y se acerca al MLP.\n",
    "\n",
    "El LSTM mejora el balance entre sensibilidad (recall) y precisión para los casos \"No normales\".\n",
    "\n",
    "Tiene una predicción más justa y estable que el CNN.\n",
    "\n",
    "Si nuestro objetivo fuera detectar niveles elevados de monóxido sin perder demasiados casos, este modelo es muy competitivo, al nivel del MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ventanas CNN-LSTM (forma 4D: muestras, subseq, pasos, features) ---\n",
    "def crear_ventanas_cnn_lstm(X, y, window_size, subseq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window_size, len(y)):\n",
    "        # secuencia completa de longitud window_size\n",
    "        full_seq = X[i - window_size : i, :]  \n",
    "        # divido en subseqs de subseq_len pasos\n",
    "        subseqs = full_seq.reshape((window_size // subseq_len,\n",
    "                                    subseq_len,\n",
    "                                    X.shape[1]))\n",
    "        Xs.append(subseqs)\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "window_size = 24\n",
    "subseq_len = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7466, 6, 4, 9)\n",
      "y_train shape: (7466,)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9248 - loss: 0.2965 - val_accuracy: 0.9330 - val_loss: 0.2460\n",
      "Epoch 2/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9402 - loss: 0.2082 - val_accuracy: 0.9330 - val_loss: 0.2083\n",
      "Epoch 3/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9405 - loss: 0.1982 - val_accuracy: 0.9347 - val_loss: 0.1964\n",
      "Epoch 4/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9419 - loss: 0.1840 - val_accuracy: 0.9368 - val_loss: 0.1782\n",
      "Epoch 5/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9413 - loss: 0.1712 - val_accuracy: 0.9384 - val_loss: 0.1774\n",
      "Epoch 6/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9452 - loss: 0.1602 - val_accuracy: 0.9400 - val_loss: 0.1733\n",
      "Epoch 7/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9394 - loss: 0.1669 - val_accuracy: 0.9400 - val_loss: 0.1631\n",
      "Epoch 8/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9379 - loss: 0.1695 - val_accuracy: 0.9416 - val_loss: 0.1595\n",
      "Epoch 9/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9393 - loss: 0.1615 - val_accuracy: 0.9400 - val_loss: 0.1591\n",
      "Epoch 10/10\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9425 - loss: 0.1547 - val_accuracy: 0.9438 - val_loss: 0.1579\n"
     ]
    }
   ],
   "source": [
    "X_seq, y_seq = crear_ventanas_cnn_lstm(X_scaled, y_bin, window_size, subseq_len)\n",
    "# ahora X_seq.shape = (n_samples, 6, 4, n_features)\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Verificar shapes\n",
    "print(\"X_train shape:\", X_train.shape)  \n",
    "# debería mostrar (n_train, 6, 4, n_features)\n",
    "print(\"y_train shape:\", y_train.shape)  \n",
    "\n",
    "# 6. Definir CNN-LSTM\n",
    "n_subseq, n_steps, n_features = X_train.shape[1], X_train.shape[2], X_train.shape[3]\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv1D(64, 2, activation='relu'),\n",
    "                    input_shape=(n_subseq, n_steps, n_features)),\n",
    "    TimeDistributed(MaxPooling1D(2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 7. Compilar\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 8. Entrenar\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación CNN-LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      1.00      0.97      1742\n",
      "   No normal       0.79      0.22      0.34       125\n",
      "\n",
      "    accuracy                           0.94      1867\n",
      "   macro avg       0.87      0.61      0.66      1867\n",
      "weighted avg       0.94      0.94      0.93      1867\n",
      "\n",
      "Exactitud total: 0.944\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación CNN-LSTM:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo clasifica con alta precisión los casos \"Normales\", lo cual es esperado por ser la clase mayoritaria.\n",
    "\n",
    "En cambio, para los casos \"No normales\" (niveles elevados de CO):\n",
    "\n",
    "Solo detecta correctamente 25% de ellos (bajo recall).\n",
    "\n",
    "Cuando predice “No normal”, acierta el 58% de las veces (precisión aceptable).\n",
    "\n",
    "El F1-score es bajo (0.35), lo que refleja la dificultad para capturar esta clase minoritaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_flat shape: (7466, 216)\n",
      "Test accuracy RF: 0.9534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Crear ventanas  (ya lo habías hecho) ---\n",
    "window_size = 24\n",
    "subseq_len  = 4\n",
    "X_seq, y_seq = crear_ventanas_cnn_lstm(X_scaled, y_bin, window_size, subseq_len)\n",
    "# X_seq.shape → (n_samples, n_subseq, n_steps, n_features)\n",
    "\n",
    "# --- 2. Train/Test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3. Aplanar cada muestra a 2D ---\n",
    "n_samples, n_subseq, n_steps, n_features = X_train.shape\n",
    "X_train_flat = X_train.reshape(n_samples, n_subseq * n_steps * n_features)\n",
    "X_test_flat  = X_test.reshape(X_test.shape[0],\n",
    "                              n_subseq * n_steps * n_features)\n",
    "\n",
    "print(\"X_train_flat shape:\", X_train_flat.shape)  \n",
    "# debería ser (n_train, 6*4*n_features)\n",
    "\n",
    "# --- 4. Entrenar Random Forest ---\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_flat, y_train)\n",
    "\n",
    "# --- 5. Evaluar ---\n",
    "acc = rf.score(X_test_flat, y_test)\n",
    "print(f\"Test accuracy RF: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Aplanar X_test igual que X_train_flat ---\n",
    "n_samples_test = X_test.shape[0]\n",
    "X_test_flat = X_test.reshape(n_samples_test,\n",
    "                             n_subseq * n_steps * n_features)\n",
    "# o directamente\n",
    "# X_test_flat = X_test.reshape(n_samples_test, -1)\n",
    "\n",
    "# --- 2. Predecir usando el RF ya entrenado ---\n",
    "y_pred = rf.predict(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación — Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.96      0.99      0.98      1742\n",
      "   No normal       0.82      0.39      0.53       125\n",
      "\n",
      "    accuracy                           0.95      1867\n",
      "   macro avg       0.89      0.69      0.75      1867\n",
      "weighted avg       0.95      0.95      0.95      1867\n",
      "\n",
      "Exactitud total: 0.953\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación — Random Forest:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tiene muy alto desempeño para la clase Normal (como los otros modelos).\n",
    "\n",
    "Para la clase No normal:\n",
    "\n",
    "Recall: 0.63 el mejor entre todos los modelos, hasta ahora\n",
    "\n",
    "Precisión: 0.59 bastante equilibrado.\n",
    "\n",
    "F1-score: 0.61 mejor resultado hasta ahora para esta clase minoritaria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Random Forest supera a todos los modelos anteriores en su capacidad para detectar casos \"No normales\". Tiene el mejor recall y F1-score para esta clase crítica, manteniendo una exactitud general muy alta. Es, por tanto, una de las mejores opciones si tu objetivo es detectar contaminación por CO(GT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Por qué Random Forest tuvo mejores resultados?\n",
    "Maneja bien desequilibrios de clase:\n",
    "Random Forest es robusto ante desequilibrios de clases. Al construir múltiples árboles sobre subconjuntos aleatorios, algunos árboles ven más casos de la clase minoritaria, lo que mejora su sensibilidad (recall) en esa clase.\n",
    "\n",
    " Modelo no secuencial: \n",
    "A diferencia de LSTM o CNN-LSTM, que dependen del orden temporal y pueden diluir señales débiles de eventos raros, el Random Forest ve cada instante como un punto independiente y se enfoca en la relación entre variables en ese momento, lo cual puede capturar mejor señales puntuales de contaminación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora decidimos hacer un xgboost por que el xgboost inicia con un árbol pequeño y va corrigiendo los errores hasta mejorar poco a poco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Entrenar modelo XGBoost ---\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100, #número de árboles\n",
    "    max_depth=10, #profundidad de cada árbol\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=(sum(y_train == 0) / sum(y_train == 1)),  # balancear clases\n",
    "    use_label_encoder=False, # no advertencias de xgboost\n",
    "    eval_metric='logloss', #log loss para binario\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"Reporte de clasificación — XGBoost:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase \"Normal\" (0):\n",
    "Muy alta precisión (0.98): casi nunca predice “Normal” cuando no lo es.\n",
    "\n",
    "Muy buen recall (0.92): detecta la gran mayoría de los casos normales.\n",
    "\n",
    "F1-score alto: equilibrio perfecto entre precisión y sensibilidad.\n",
    "\n",
    "Clase \"No normal\" (1):\n",
    "Precisión moderada (0.39): se equivoca a veces cuando dice “No normal”.\n",
    "\n",
    "Recall excelente (0.78): detecta casi 8 de cada 10 episodios reales de contaminación. No esta nada mal\n",
    "\n",
    "F1-score de 0.52: mejor que cualquier otro modelo anterior en esta clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la exactitud total es más baja (91.2%), el modelo XGBoost es el mejor en detectar episodios \"No normales\", que son los más importantes y difíciles de predecir. Su alto recall (0.78) lo convierte en una herramienta excelente para alertas tempranas, aunque a costa de más falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito fue transformar el problema de regresión de predicción de CO(GT) en una tarea de clasificación binaria, donde la clase \"Normal\" incluye niveles de CO menores o iguales a 4.4 mg/m³ (según la guía de calidad del aire de la OMS), y la clase \"No normal\" agrupa los valores por encima de ese umbral. La meta principal fue evaluar qué modelos detectan mejor los casos \"No normales\", que representan condiciones potencialmente peligrosas para la salud.\n",
    "\n",
    "#### Modelos realizados\n",
    "\n",
    "| Modelo        | Accuracy | Recall (No normal) | F1-score (No normal) |\n",
    "| ------------- | -------- | ------------------ | -------------------- |\n",
    "| MLP           | 0.950    | 0.54               | 0.57                 |\n",
    "| CNN           | 0.952    | 0.27               | 0.41                 |\n",
    "| LSTM          | 0.951    | 0.50               | 0.55                 |\n",
    "| CNN-LSTM      | 0.944    | 0.25               | 0.35                 |\n",
    "| Random Forest | 0.951    | 0.63               | 0.61                 |\n",
    "| XGBoost       | 0.912    | **0.78**           | **0.52**             |\n",
    "\n",
    "\n",
    "- Todos los modelos neuronales tuvieron alta precisión general, especialmente en la clase \"Normal\", pero varios presentaron bajo recall en la clase \"No normal\", lo que indica que fallan en detectar eventos de contaminación.\n",
    "- El modelo XGBoost, aunque tuvo menor exactitud general, alcanzó el mejor recall (0.78) para la clase \"No normal\". Esto lo convierte en el más útil para sistemas de alerta temprana, donde detectar correctamente niveles elevados es más importante que equivocarse ocasionalmente.\n",
    "La arquitectura y los hiperparámetros del XGBoost se ajustaron para manejar el desbalance de clases usando el parámetro scale_pos_weight. Puede explicar por que el mojoró el resultado\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos neuronles con oversampling para el desbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de clase: {0: np.float64(0.5328290037111048), 1: np.float64(8.115217391304348)}\n"
     ]
    }
   ],
   "source": [
    "#balance de las clases\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Pesos de clase:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    # aplanamos 4D→2D\n",
    "    Flatten(input_shape=(X_train.shape[1],\n",
    "                         X_train.shape[2],\n",
    "                         X_train.shape[3])),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.8213 - loss: 0.3383 - val_accuracy: 0.8594 - val_loss: 0.3102\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - accuracy: 0.8172 - loss: 0.3253 - val_accuracy: 0.6058 - val_loss: 0.7611\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - accuracy: 0.8082 - loss: 0.3345 - val_accuracy: 0.6968 - val_loss: 0.5729\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.7956 - loss: 0.3327 - val_accuracy: 0.8106 - val_loss: 0.3597\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - accuracy: 0.8466 - loss: 0.2887 - val_accuracy: 0.8916 - val_loss: 0.2158\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - accuracy: 0.8546 - loss: 0.2998 - val_accuracy: 0.8394 - val_loss: 0.3219\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - accuracy: 0.8316 - loss: 0.3055 - val_accuracy: 0.8527 - val_loss: 0.3036\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - accuracy: 0.8447 - loss: 0.2975 - val_accuracy: 0.8414 - val_loss: 0.3004\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - accuracy: 0.8289 - loss: 0.2942 - val_accuracy: 0.8307 - val_loss: 0.3440\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - accuracy: 0.8477 - loss: 0.2628 - val_accuracy: 0.7979 - val_loss: 0.4202\n",
      "Test loss: 0.1989, Test accuracy: 0.9025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, mlflow_callback],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluación\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación MLP (balanceado):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.98      0.91      0.95      1742\n",
      "   No normal       0.39      0.80      0.52       125\n",
      "\n",
      "    accuracy                           0.90      1867\n",
      "   macro avg       0.69      0.85      0.73      1867\n",
      "weighted avg       0.94      0.90      0.92      1867\n",
      "\n",
      "Exactitud total: 0.903\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación MLP (balanceado):\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación mlp sin balanceo y con\n",
    "| Métrica               | MLP original | MLP balanceado |\n",
    "| --------------------- | ------------ | -------------- |\n",
    "| Accuracy              | 0.950        | 0.863          |\n",
    "| Recall (No normal)    | 0.54         | **0.83** mucho mejor    |\n",
    "| F1 (No normal)        | 0.57         | **0.42** ↓     |\n",
    "| Precisión (No normal) | 0.53         | **0.28** ↓     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo balanceado sacrificó exactitud general, pero a cambio logró detectar correctamente la gran mayoría de los casos \"No normal\" (83% de recall, el más alto de todos tus MLPs).\n",
    "\n",
    "La precisión bajó bastante (0.28), lo que significa que ahora el modelo lanza más falsos positivos (predice \"No normal\" cuando no lo es).\n",
    "\n",
    "El MLP con class_weight mejoró muchísimo la sensibilidad al detectar niveles peligrosos de CO (83% de recall), lo que es clave para sistemas de alerta. Aunque pierde precisión, es preferible si nuestra prioridad es no dejar pasar ningún caso “No normal”. Como todo tiene sus pros y sus contras. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Balanceada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de clase: {0: np.float64(0.5328290037111048), 1: np.float64(8.115217391304348)}\n"
     ]
    }
   ],
   "source": [
    "# Calcular pesos de clase \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Pesos de clase:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_ventanas_cnn(X, y, window_size):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window_size, len(y)):\n",
    "        Xs.append(X[i-window_size:i, :])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "window_size = 24\n",
    "\n",
    "X_seq, y_seq = crear_ventanas_cnn(X_scaled, y_bin, window_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1],\n",
    "                                                  X_train.shape[2])),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')   # binaria\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6833 - loss: 0.5944 - val_accuracy: 0.9009 - val_loss: 0.2749\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.7850 - loss: 0.4522 - val_accuracy: 0.8266 - val_loss: 0.3597\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - accuracy: 0.8402 - loss: 0.3270 - val_accuracy: 0.8206 - val_loss: 0.3733\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8412 - loss: 0.3075 - val_accuracy: 0.8226 - val_loss: 0.3817\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8531 - loss: 0.2860 - val_accuracy: 0.8701 - val_loss: 0.2682\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8502 - loss: 0.2859 - val_accuracy: 0.8454 - val_loss: 0.3067\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.8654 - loss: 0.2646 - val_accuracy: 0.8869 - val_loss: 0.2575\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.8791 - loss: 0.2365 - val_accuracy: 0.8474 - val_loss: 0.3105\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - accuracy: 0.8603 - loss: 0.2638 - val_accuracy: 0.8574 - val_loss: 0.3095\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - accuracy: 0.8393 - loss: 0.2653 - val_accuracy: 0.8762 - val_loss: 0.2461\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.8580 - loss: 0.2458 - val_accuracy: 0.8467 - val_loss: 0.3430\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.8601 - loss: 0.2484 - val_accuracy: 0.8661 - val_loss: 0.3011\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8735 - loss: 0.2196 - val_accuracy: 0.8956 - val_loss: 0.2390\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - accuracy: 0.8774 - loss: 0.2220 - val_accuracy: 0.8695 - val_loss: 0.2703\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.8873 - loss: 0.2016 - val_accuracy: 0.8614 - val_loss: 0.3030\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8749 - loss: 0.2165 - val_accuracy: 0.8407 - val_loss: 0.3590\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.8620 - loss: 0.2415 - val_accuracy: 0.8842 - val_loss: 0.2590\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.8899 - loss: 0.2035 - val_accuracy: 0.8407 - val_loss: 0.3696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x324f14410>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EarlyStopping \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenamiento \n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, mlflow_callback],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluación\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación CNN (balanceado):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.98      0.90      0.94      1742\n",
      "   No normal       0.35      0.78      0.48       125\n",
      "\n",
      "    accuracy                           0.89      1867\n",
      "   macro avg       0.67      0.84      0.71      1867\n",
      "weighted avg       0.94      0.89      0.91      1867\n",
      "\n",
      "Exactitud total: 0.889\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación CNN (balanceado):\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "| Métrica               | CNN original | CNN balanceado |\n",
    "| --------------------- | ------------ | -------------- |\n",
    "| Accuracy              | 95.2%        | 86.6%          |\n",
    "| Recall (No normal)    | 0.27         | **0.81** mejor   |\n",
    "| Precisión (No normal) | 0.82         | **0.29** ↓     |\n",
    "| F1-score (No normal)  | 0.41         | **0.42** ↔     |\n",
    "\n",
    "El recall de la clase “No normal” pasó de 27% a 81%, una mejora enorme, lo que significa que ahora el modelo sí detecta la mayoría de los episodios de CO elevados.\n",
    "\n",
    "La precisión bajó, como es natural cuando el modelo predice más casos positivos (hay más falsos positivos).\n",
    "\n",
    "El F1-score se mantuvo, pero con un perfil distinto: más sensibilidad, menos precisión.\n",
    "\n",
    "Accuracy bajó, porque se sacrifica algo de rendimiento en la clase mayoritaria (\"Normal\") para detectar mejor la minoritaria.\n",
    "\n",
    "Al usar class_weight, la CNN se vuelve mucho más útil para detectar niveles peligrosos de CO, con un recall de 81% que la convierte en un modelo efectivo para aplicaciones de monitoreo o alerta. Aunque su precisión baja, esto es aceptable si tu objetivo es no dejar pasar casos de contaminación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de clase: {0: np.float64(0.5328290037111048), 1: np.float64(8.115217391304348)}\n"
     ]
    }
   ],
   "source": [
    "# balanceo\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Pesos de clase:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Modelo LSTM \n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5243 - loss: 0.6578 - val_accuracy: 0.8534 - val_loss: 0.4785\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7871 - loss: 0.5101 - val_accuracy: 0.8715 - val_loss: 0.3191\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8246 - loss: 0.4087 - val_accuracy: 0.8614 - val_loss: 0.3514\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8573 - loss: 0.3654 - val_accuracy: 0.8019 - val_loss: 0.4760\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8523 - loss: 0.3170 - val_accuracy: 0.9036 - val_loss: 0.2394\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8709 - loss: 0.3069 - val_accuracy: 0.8226 - val_loss: 0.4017\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8628 - loss: 0.2712 - val_accuracy: 0.7845 - val_loss: 0.4788\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8605 - loss: 0.2850 - val_accuracy: 0.8648 - val_loss: 0.3040\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8600 - loss: 0.2916 - val_accuracy: 0.8487 - val_loss: 0.3791\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8551 - loss: 0.2769 - val_accuracy: 0.8655 - val_loss: 0.3011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x325974cb0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EarlyStopping \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenamiento con pesos de clase \n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, mlflow_callback],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación LSTM (balanceado):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.98      0.92      0.95      1742\n",
      "   No normal       0.43      0.78      0.55       125\n",
      "\n",
      "    accuracy                           0.91      1867\n",
      "   macro avg       0.70      0.85      0.75      1867\n",
      "weighted avg       0.95      0.91      0.93      1867\n",
      "\n",
      "Exactitud total: 0.915\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación LSTM (balanceado):\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo detecta correctamente 80% de los casos \"No normal\" → excelente mejora de recall.\n",
    "\n",
    "Precisión de la clase minoritaria (0.39) es razonable, considerando el desbalance y que el modelo intenta no dejar pasar alertas.\n",
    "\n",
    "El modelo sigue teniendo muy buen desempeño en la clase \"Normal\" (recall de 92% y F1 de 0.95).\n",
    "\n",
    "Accuracy general de 91% lo pone al nivel del XGBoost, pero con la ventaja de usar la secuencia temporal.\n",
    "\n",
    "Tu LSTM balanceado es uno de los mejores modelos para detectar condiciones de CO elevadas, logrando un excelente balance entre recall alto (80%) y exactitud total (91%), algo que los otros modelos neuronales no alcanzaban sin perder mucha precisión.\n",
    "Además, aprovecha la estructura secuencial del problema, lo cual le da una ventaja conceptual sobre los modelos clásicos como XGBoost o RF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de clase: {0: np.float64(0.5328290037111048), 1: np.float64(8.115217391304348)}\n"
     ]
    }
   ],
   "source": [
    "# balanceo\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Pesos de clase:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7466, 6, 4, 9)\n",
      "y_train shape: (7466,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daftamayo/Downloads/Air_Quality_Pred_Clas-main/.vnev/lib/python3.12/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "def crear_ventanas_cnn_lstm(X, y, window_size, subseq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window_size, len(y)):\n",
    "        full_seq = X[i-window_size:i, :]  \n",
    "        subseqs = full_seq.reshape((window_size // subseq_len,\n",
    "                                    subseq_len,\n",
    "                                    X.shape[1]))\n",
    "        Xs.append(subseqs)\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "window_size = 24      \n",
    "subseq_len  = 4       \n",
    "\n",
    "X_seq, y_seq = crear_ventanas_cnn_lstm(X_scaled, y_bin, window_size, subseq_len)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  \n",
    "print(\"y_train shape:\", y_train.shape)  \n",
    "\n",
    "n_subseq, n_steps, n_features = X_train.shape[1], X_train.shape[2], X_train.shape[3]\n",
    "model = Sequential([\n",
    "    TimeDistributed(\n",
    "        Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "        input_shape=(n_subseq, n_steps, n_features)\n",
    "    ),\n",
    "    TimeDistributed(MaxPooling1D(pool_size=2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4194 - loss: 0.6693 - val_accuracy: 0.4364 - val_loss: 0.7547\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5843 - loss: 0.5910 - val_accuracy: 0.6352 - val_loss: 0.5784\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6379 - loss: 0.5838 - val_accuracy: 0.7195 - val_loss: 0.5831\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7071 - loss: 0.5389 - val_accuracy: 0.7838 - val_loss: 0.4340\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7641 - loss: 0.4832 - val_accuracy: 0.8434 - val_loss: 0.3549\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7775 - loss: 0.4398 - val_accuracy: 0.7503 - val_loss: 0.4501\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7880 - loss: 0.4090 - val_accuracy: 0.7162 - val_loss: 0.5285\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7558 - loss: 0.4335 - val_accuracy: 0.7637 - val_loss: 0.4474\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8007 - loss: 0.3924 - val_accuracy: 0.7497 - val_loss: 0.4571\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8066 - loss: 0.3743 - val_accuracy: 0.7296 - val_loss: 0.5555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3266f8770>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, mlflow_callback],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación CNN-LSTM (balanceado):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.98      0.85      0.91      1742\n",
      "   No normal       0.26      0.75      0.39       125\n",
      "\n",
      "    accuracy                           0.84      1867\n",
      "   macro avg       0.62      0.80      0.65      1867\n",
      "weighted avg       0.93      0.84      0.87      1867\n",
      "\n",
      "Exactitud total: 0.840\n"
     ]
    }
   ],
   "source": [
    "print(\"Reporte de clasificación CNN-LSTM (balanceado):\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"No normal\"]))\n",
    "print(f\"Exactitud total: {accuracy_score(y_test, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo sigue siendo muy fuerte para la clase \"Normal\", con una alta precisión (0.98) y buen recall (0.88).\n",
    "\n",
    "Para la clase \"No normal\", el modelo logra un recall de 65%, lo que significa que detecta 2 de cada 3 episodios de contaminación (CO alto).\n",
    "\n",
    "La precisión de 0.26 indica que hay bastantes falsos positivos, es decir, el modelo a veces predice \"No normal\" cuando no lo es.\n",
    "\n",
    "El F1-score de 0.38 para la clase minoritaria muestra que hay un esfuerzo efectivo pero limitado para balancear detección vs falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo CNN-LSTM mejora notablemente el recall respecto al CNN original, pero no logra superar al LSTM balanceado, que sigue siendo el mejor modelo en balance entre detección de eventos peligrosos y estabilidad.\n",
    "Este modelo aún es útil si se busca un sistema más general que combine detección local (CNN) y secuencial (LSTM), pero se queda corto en precisión al detectar CO elevado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin balanceo, los modelos neuronales mostraban alta exactitud general, pero muy bajo recall en la clase minoritaria (“No normal”), lo que significa que omitían la mayoría de los episodios de contaminación.\n",
    "\n",
    "Al aplicar class_weight para dar mayor importancia a la clase minoritaria durante el entrenamiento:\n",
    "\n",
    "Se logró un aumento significativo del recall para “No normal” (por ejemplo, de 27% a 81% en CNN) mucha diferencia.\n",
    "\n",
    "Se observó una reducción aceptable en la precisión general, pero con un gran beneficio en la sensibilidad del modelo ante condiciones de riesgo.\n",
    "\n",
    "El LSTM balanceado fue el modelo con mejor desempeño general: alto recall (80%) en la clase crítica, mejor F1-score para “No normal” (0.52) y una exactitud total sólida del 91.2%. Este modelo ofrece la mejor combinación entre detección efectiva de episodios contaminantes y estabilidad en las predicciones.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vnev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
